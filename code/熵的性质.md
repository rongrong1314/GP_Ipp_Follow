#### 香农熵的三个性质：

   性质1：$H(P_{1},...,P_{a})$是向量$p=(p_{1},...,p_{a})$的非负连续函数。

   性质2：对于任何正整数a，必有$g(a)<g(a+1)$成立。（数越多，熵越大）

   性质3：对任意一组正整数$b_{i},i=1,2,....,k$，如果$\sum_{i=1}^{k} b_{i}=a$，那么

   $g(a)=H\left(b_{1}^{\prime}, b_{2}^{\prime}, \cdots, b_{k}^{\prime}\right)+\sum_{i=1}^{k} b_{i}^{\prime} g\left(b_{i}\right)$

   其中：$b_{i}^{\prime}=b_{i}/a$。

   性质3说明了总体不确定性与局部不确定性的关系。可以形象的描述为，如果总体是某系全体二年级学生，那么它的不确定性为：

   $二年级全体学生的不确定性=班级的不确定性+在某个班级中平均不确定性$

   如果函数满足上述三条性质，那么必有：

​    $H\left(p_{1}, \cdots, p_{a}\right)=-\sum_{i=1}^{a} p_{i} \log _{c} p_{i}$              (1)

  其中$c>0$，且$0log _{c} 0=0$。

####   联合熵：

   $H(X, Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)$                  (2)

​     数学期望：

​      $E[g(X, Y)]=\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} g(x, y) p(x, y)$             (3)

   如果随机变量相互独立，存在

   $H(X, Y)=H(X)+H(Y)=-\sum_{x \in \mathcal{X}} p(x) \log p(x)+\sum_{y \in \mathcal{Y}} q(y) \log q(y)$              (4)

   其中：

​    $p(x)=P_{r}\{X=x\}=\sum_{y \in \mathcal{Y}} p(x, y), q(y)=P_{r}\{Y=y\}=\sum_{x \in \mathcal{X}} p(x, y)$为随机概率分布或边际分布。

####  条件熵：

联合分布存在条件概率分布：

$\left\{\begin{array}{l}
q(y | x)=\frac{p(x, y)}{p(x)}, \text { 当 } p(x) \neq 0 \text { 时 } \\
p(x | y)=\frac{p(x, y)}{q(y)}, \text { 当 } q(y) \neq 0 \text { 时 }
\end{array}\right.$                       (5)

条件熵是一个随机变量在另一个随机变量给定的条件下的平均不确定性，它们定义为：

$\left\{\begin{array}{l}
H(Y | X)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log q(y | x) \\
H(X | Y)=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x | y)
\end{array}\right.$             (6)

#### 一些定义：

​          对条件熵。总有$H(X | Y) \geq 0$成立，且等号成立的充要条件是X是一个由Y决定的随机变量。这就是条件分布$p(x|y)$只取0或1。

​          联合熵与条件熵的关系为

​         $H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)$          (7)

​      该定理可以由条件熵与条件概率分布定义直接得到。

​      马尔科夫链：

​             $p\left(\left(x_{1}, x_{2}, \cdots, x_{i-1}\right), x_{i+1} | x_{i}\right)=p\left(x_{1}, x_{2}, \cdots, x_{i-1} | x_{i}\right) p\left(x_{i+1} | x_{i}\right)$                (8)

​             $H\left(X_{1}, X_{2}, \cdots, X_{a}\right)=\sum_{i=1}^{a} H\left(X_{i} | X_{i-1}\right)$                               (9)

####     熵的基本性质：

#####        不等式：

​     (1)如果$\sum_{i=1}^{a} p_{i}=\sum_{i=1}^{a} q_{i}=1$，那么

​      $-\sum_{i=1}^{a} p_{i} \log p_{i} \leq-\sum_{i=1}^{a} p_{i} \log q_{i}$

​      又可以写作$\sum_{i=1}^{a} p_{i} \log \frac {p_{i}}{q_{i}}\geq 0$。

​    (2)对$u$和$v$，记$u=\sum_{i=1}^{a} u_{i}, v=\sum_{i=1}^{a} v_{i}$，那么有

​    $\sum_{i=1}^{a} u_{i} \log \frac{u_{i}}{v_{i}} \geq u \log \frac{u}{v}$

​     (3)如果X是个离散随机变量，在$\mathcal{X}=\left\{x_{1}, x_{2}, \cdots, x_{a}\right\}$中取值，具有概率分布为$p$，那么

​        $loga \geq H(X)$

   其中等号成立的充要条件是对所有的$i$都有$p_{i}= \frac {1}{a}$成立。

#####      熵函数的可加性：

​      存在两个非负向量，满足$\sum_{j=1}^{k_{i}} q_{i j}=p_{i}, i=1,2, \cdots, a$，且$\sum_{i=1}^{a}p_{i}=1$。那么以下关系式成立：

​         $H(\boldsymbol{Q})=H(\boldsymbol{p})+\sum_{i=1}^{a} p_{i} H\left(\boldsymbol{q}_{i}^{\prime}\right)$

​       其中$q_{i j}^{\prime}=\frac{q_{i j}}{p_{i}}$。

##### Fano和Jenson不等式：

######    Fano：

  其给出了两个随机变量的条件熵与误差之间的关系。

  如果X和Y是在$\mathcal{X}$中取值的随机变量，记它们的联合分布为$p(x,y)$。如记$p_{e}=P_{r}\{X \neq Y\}$为X与Y不相同的概率，那么存在

​    $H(X | Y) \leq H\left(p_{e}\right)+p_{e} \log (a-1)$

  其中$a=||\mathcal{X}||$为集合$\mathcal{X}$的元素个数，$H(p)$为熵函数。

######    Jenson：

 定义1：称函数$g(x)$在区间$(a,b)$上是上凸的，如果对任意的$x_{1}, x_{2} \in(a,b)$和$0 \leq \lambda \leq 1$，都有

 $g\left(\lambda x_{1}+(1-\lambda) x_{2}\right) \geq \lambda g\left(x_{1}\right)+(1-\lambda) g\left(x_{2}\right)$

如果等号只有在$\lambda=0$或$\lambda=1$，或$x_{1}=x_{2}$时才成立，则称函数g是严格上凸的。如果定义的不等式相反，则为下凸函数。

定义2：如果$g$是一个上凸函数而X是一个随机变量，则有

  $E[g(X)] \geq g[E(X)]$

如果g是严格上凸的，那么等号成立的充要条件是X以概率1取为常数。

#### 互熵和互信息：

##### 互熵：

设$p(x),q(x)$是$\mathcal{X}$中取值的两个概率分布，那么它们的互熵定义为

$K(p ; q)=\sum_{i=1}^{a} p(x) \log \frac{p(x)}{q(x)}$

互熵又被称为Kullback-Leibler散度（Divergence）,或Kullback-Leibler距离。互熵的最大优点是可以推广到任意概率空间。

互熵的基本性质是：对任何$\mathcal{X}$上的两个概率分布$p(x),q(x)$总有$K(p;q)\geq0$，其中等号成立的充要条件是$p_{i}=q_{i}$。

如果q固定，K是关于p的下凸函数，这时对任何不同的概率分布，总有

$K\left(\lambda p_{1}+(1-\lambda) p_{2} ; q\right) \leq \lambda K\left(p_{1} ; q\right)+(1-\lambda) K\left(p_{2} ; q\right)$

如果p固定，那么K关于q是下凸函数，这时对任何不同的概率分布总有

$K\left(p ; \lambda q_{1}+(1-\lambda) q_{2}\right) \leq \lambda K\left(p ; q_{1}\right)+(1-\lambda) K\left(p ; q_{2}\right)$

如果K关于p，q是下凸函数，对于任何不同的概率分布总有

  $K\left(\lambda p_{1}+(1-\lambda) p_{2} ; \lambda q_{1}+(1-\lambda) q_{2}\right) \leq \lambda K\left(p_{1} ; q_{1}\right)+(1-\lambda) K\left(p_{2} ; q_{2}\right)$

##### 互信息：

互信息是一种特殊的互熵，它定义为：

$I(X ; Y)=\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) q(y)}$

​     



   

   







